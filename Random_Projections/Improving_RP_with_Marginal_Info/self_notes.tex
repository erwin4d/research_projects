\documentclass[a4paper,12pt]{article} 
\usepackage{amsmath,amssymb, amsthm, mathrsfs, fancyhdr, ulem, gastex, gensymb, harmony, color, enumitem, bm, hyperref, extarrows, makeidx, rotating, wasysym, calligra}
\usepackage[encapsulated]{CJK}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
% use one of bsmi(trad Chinese), gbsn(simp Chinese), min(Japanese), mj(Korean); see:
% /usr/share/texmf-dist/tex/latex/cjk/texinput/UTF8/*.fd
\usepackage{tikz} 
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit}  
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}} 
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}} 


\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0in}
\setlength\textheight{9.0in}
\setlength\textwidth{6.5in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\setlength\parindent{0.0in}
\setlength\parskip{0.25in} 

\pagestyle{fancy}
\headheight 15pt
\headsep 20pt

\def\newop#1 
{\expandafter\def\csname #1 
\endcsname{\mathop{\rm #1}\nolimits}} 
%Use as \newop{Blah}, then \Blah works. Similar to say, \dim, or \min, etcc

\lhead[\thepage]{Some notes for Improving RP with Marginal Info} 
\rhead[Some notes for Improving RP with Marginal Info]{\thepage} 	
\chead{}
\cfoot{}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{question}{Question}[section]
\newtheorem{process}{Process}[section]
\newtheorem{skill}{Skill}[section]
\newtheorem{result}{Result}[section]
\newtheorem{remark}{Remark}[section]
\numberwithin{equation}{section}
\newcommand{\cntext}[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}

\makeindex
\begin{document} 

Bounds for the estimator are in the paper, but here's a reminder of what this estimator is doing.

Suppose we compute $V = \frac{1}{\sqrt{k}}XR$, and look at any two rows $i,j$ of $V$. Each tuple $(v_{ik}, v_{jk})$ are seen as bivariate normals, i.e.
\begin{align*}
\left(\begin{array}{c}
v_{ik} \\
v_{jk}
\end{array}\right) \sim N\left( {\mu}, \Sigma  \right)
\end{align*}
with:
\begin{align*}
\mu & = (0,0) \\
\Sigma & = \frac{1}{K} \left(\begin{array}{c c}
m_i & a \\
a & m_j
\end{array}\right)
\end{align*}
with $a$ denoting the true inner product between ${\bf x}_i, {\bf x}_j$, and $m_i, m_j$ denoting the norms of $\|{\bf x}_i\|_2^2$, $\|{\bf x}_j\|_2^2$ respectively.

For $K$ columns of $R$, we see $K$ such observations, and thus the likelihood is simply proportional to:
\begin{align*}
L( (v_{ik}, v_{jk})) \propto | \Sigma |^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2} \sum_{k=1}^K \left( (v_{ik} ~~ v_{jk}) \Sigma^{-1} \left(\begin{array}{c}
v_{ik} \\
v_{jk}
\end{array}\right)   \right) \right\}
\end{align*}

The trick is to express the likelihood function in terms of $a$, the inner product, and find $\hat{a}$ which maximizes this likelihood.

The loglikelihood (derivation in the paper) is given by:
\begin{align*}
l(a) & = -\frac{K}{2} \log(m_im_j - a^2) - \frac{K}{2} \frac{1}{m_im_j - a^2} \sum_{k=1}^K(v_{ik}^2m_j - 2v_{ik}v_{jk}a + v_{jk}^2m_i)
\end{align*}

Thus, getting the MLE of $a$ is equivalent to equating $l'(a) = 0$, and finding $\hat{a}$ which gives $l'(\hat{a}) = 0$. Thus, need root finding code to do this. $\hat{a}$ solution to:
\begin{align*}
a^3 - a^2({\bf v}_i^T{\bf v}_j) + a(-m_im_j + m_i\|{\bf v}_j\|_2^2 + m_j \|{\bf v}_i\|_2^2) - m_im_j {\bf v}_i^T{\bf v}_j = 0
\end{align*}







\end{document}



